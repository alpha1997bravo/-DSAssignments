{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a03069",
   "metadata": {},
   "source": [
    "# TEXT CLASSIFICATION USING NAIVE BAYES AND SENTIMENT ANALYSIS ON BLOG POSTS\n",
    "Overview\n",
    "In this assignment, you will work on the \"blogs_categories.csv\" dataset, which contains blog posts categorized into various themes. Your task will be to build a text classification model using the Naive Bayes algorithm to categorize the blog posts accurately. Furthermore, you will perform sentiment analysis to understand the general sentiment (positive, negative, neutral) expressed in these posts. This assignment will enhance your understanding of text classification, sentiment analysis, and the practical application of the Naive Bayes algorithm in Natural Language Processing (NLP).\n",
    "Dataset\n",
    "The provided dataset, \"blogs_categories.csv\", consists of blog posts along with their associated categories. Each row represents a blog post with the following columns:\n",
    "•\tText: The content of the blog post. Column name: Data\n",
    "•\tCategory: The category to which the blog post belongs. Column name: Labels\n",
    "Tasks\n",
    "1. Data Exploration and Preprocessing\n",
    "•\tLoad the \"blogs_categories.csv\" dataset and perform an exploratory data analysis to understand its structure and content.\n",
    "•\tPreprocess the data by cleaning the text (removing punctuation, converting to lowercase, etc.), tokenizing, and removing stopwords.\n",
    "•\tPerform feature extraction to convert text data into a format that can be used by the Naive Bayes model, using techniques such as TF-IDF.\n",
    "2. Naive Bayes Model for Text Classification\n",
    "•\tSplit the data into training and test sets.\n",
    "•\tImplement a Naive Bayes classifier to categorize the blog posts into their respective categories. You can use libraries like scikit-learn for this purpose.\n",
    "•\tTrain the model on the training set and make predictions on the test set.\n",
    "3. Sentiment Analysis\n",
    "•\tChoose a suitable library or method for performing sentiment analysis on the blog post texts.\n",
    "•\tAnalyze the sentiments expressed in the blog posts and categorize them as positive, negative, or neutral. Consider only the Data column and get the sentiment for each blog.\n",
    "•\tExamine the distribution of sentiments across different categories and summarize your findings.\n",
    "4. Evaluation\n",
    "•\tEvaluate the performance of your Naive Bayes classifier using metrics such as accuracy, precision, recall, and F1-score.\n",
    "•\tDiscuss the performance of the model and any challenges encountered during the classification process.\n",
    "•\tReflect on the sentiment analysis results and their implications regarding the content of the blog posts.\n",
    "Submission Guidelines\n",
    "•\tYour submission should include a comprehensive report and the complete codebase.\n",
    "•\tYour code should be well-documented and include comments explaining the major steps.\n",
    "Evaluation Criteria\n",
    "•\tCorrect implementation of data preprocessing and feature extraction.\n",
    "•\tAccuracy and robustness of the Naive Bayes classification model.\n",
    "•\tDepth and insightfulness of the sentiment analysis.\n",
    "•\tClarity and thoroughness of the evaluation and discussion sections.\n",
    "•\tOverall quality and organization of the report and code.\n",
    "Good luck, and we look forward to your insightful analysis of the blog posts dataset!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5915bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing basic labraries\n",
    "# and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a433e1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               Data       Labels\n",
      "0           0  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:49...  alt.atheism\n",
      "1           1  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:51...  alt.atheism\n",
      "2           2  Newsgroups: alt.atheism\\nPath: cantaloupe.srv....  alt.atheism\n",
      "3           3  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:51...  alt.atheism\n",
      "4           4  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:51...  alt.atheism\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19997 entries, 0 to 19996\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  19997 non-null  int64 \n",
      " 1   Data        19997 non-null  object\n",
      " 2   Labels      19997 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 468.8+ KB\n",
      "None\n",
      "Unnamed: 0    0\n",
      "Data          0\n",
      "Labels        0\n",
      "dtype: int64\n",
      "alt.atheism                 1000\n",
      "comp.graphics               1000\n",
      "talk.politics.misc          1000\n",
      "talk.politics.mideast       1000\n",
      "talk.politics.guns          1000\n",
      "sci.space                   1000\n",
      "sci.med                     1000\n",
      "sci.electronics             1000\n",
      "sci.crypt                   1000\n",
      "rec.sport.hockey            1000\n",
      "rec.sport.baseball          1000\n",
      "rec.motorcycles             1000\n",
      "rec.autos                   1000\n",
      "misc.forsale                1000\n",
      "comp.windows.x              1000\n",
      "comp.sys.mac.hardware       1000\n",
      "comp.sys.ibm.pc.hardware    1000\n",
      "comp.os.ms-windows.misc     1000\n",
      "talk.religion.misc          1000\n",
      "soc.religion.christian       997\n",
      "Name: Labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"blogs_categories.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Check the structure of the dataset\n",
    "print(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check the distribution of categories (labels)\n",
    "print(df['Labels'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27914212",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "Preprocessing involves cleaning the text by removing punctuation, converting text to lowercase, removing stopwords, and tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbbdf0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    xref cantaloupesrvcscmuedu altatheism altathei...\n",
      "1    xref cantaloupesrvcscmuedu altatheism altathei...\n",
      "2    newsgroups altatheism path cantaloupesrvcscmue...\n",
      "3    xref cantaloupesrvcscmuedu altatheism altpolit...\n",
      "4    xref cantaloupesrvcscmuedu altatheism socmotss...\n",
      "Name: Data, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "df['Data'] = df['Data'].apply(preprocess_text)\n",
    "\n",
    "# Display a sample of the preprocessed text\n",
    "print(df['Data'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8f667",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "We’ll use TF-IDF (Term Frequency-Inverse Document Frequency) to convert the text data into numerical features that can be used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691835ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15997, 5000) (4000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Split data into features and labels\n",
    "X = df['Data']\n",
    "y = df['Labels']\n",
    "\n",
    "# Convert text data to TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check shapes of training and testing data\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a689459",
   "metadata": {},
   "source": [
    "### Naive Bayes Model for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad76f693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8885\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.73      0.77      0.75       173\n",
      "           comp.graphics       0.78      0.89      0.84       179\n",
      " comp.os.ms-windows.misc       0.88      0.91      0.89       226\n",
      "comp.sys.ibm.pc.hardware       0.84      0.78      0.81       204\n",
      "   comp.sys.mac.hardware       0.89      0.94      0.91       205\n",
      "          comp.windows.x       0.92      0.91      0.92       186\n",
      "            misc.forsale       0.85      0.88      0.87       190\n",
      "               rec.autos       0.90      0.92      0.91       203\n",
      "         rec.motorcycles       0.98      0.93      0.96       218\n",
      "      rec.sport.baseball       0.97      0.97      0.97       192\n",
      "        rec.sport.hockey       0.99      0.98      0.98       203\n",
      "               sci.crypt       0.96      0.96      0.96       200\n",
      "         sci.electronics       0.94      0.90      0.92       227\n",
      "                 sci.med       0.99      0.95      0.97       196\n",
      "               sci.space       0.96      0.94      0.95       205\n",
      "  soc.religion.christian       0.88      1.00      0.94       215\n",
      "      talk.politics.guns       0.83      0.93      0.88       205\n",
      "   talk.politics.mideast       0.93      0.94      0.94       197\n",
      "      talk.politics.misc       0.82      0.70      0.76       200\n",
      "      talk.religion.misc       0.62      0.48      0.54       176\n",
      "\n",
      "                accuracy                           0.89      4000\n",
      "               macro avg       0.88      0.88      0.88      4000\n",
      "            weighted avg       0.89      0.89      0.89      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Instantiate the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8400c21b",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "We'll use a library like TextBlob or VADER for sentiment analysis of the blog posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7b907ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2e4f5",
   "metadata": {},
   "source": [
    "### Perform Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a76d9c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive    14272\n",
      "Negative     5707\n",
      "Neutral        18\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Function to classify sentiment\n",
    "def get_sentiment(text):\n",
    "    sentiment_score = TextBlob(text).sentiment.polarity\n",
    "    if sentiment_score > 0:\n",
    "        return 'Positive'\n",
    "    elif sentiment_score < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply sentiment analysis to the Data column\n",
    "df['Sentiment'] = df['Data'].apply(get_sentiment)\n",
    "\n",
    "# Display the sentiment distribution\n",
    "print(df['Sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1523dc5",
   "metadata": {},
   "source": [
    "### Analyze Sentiment by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90a4d1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment                 Negative  Neutral  Positive\n",
      "Labels                                               \n",
      "alt.atheism               0.286000    0.000  0.714000\n",
      "comp.graphics             0.262000    0.001  0.737000\n",
      "comp.os.ms-windows.misc   0.247000    0.000  0.753000\n",
      "comp.sys.ibm.pc.hardware  0.252000    0.002  0.746000\n",
      "comp.sys.mac.hardware     0.274000    0.000  0.726000\n",
      "comp.windows.x            0.284000    0.005  0.711000\n",
      "misc.forsale              0.229000    0.000  0.771000\n",
      "rec.autos                 0.256000    0.002  0.742000\n",
      "rec.motorcycles           0.346000    0.000  0.654000\n",
      "rec.sport.baseball        0.308000    0.001  0.691000\n",
      "rec.sport.hockey          0.347000    0.001  0.652000\n",
      "sci.crypt                 0.262000    0.000  0.738000\n",
      "sci.electronics           0.251000    0.000  0.749000\n",
      "sci.med                   0.282000    0.002  0.716000\n",
      "sci.space                 0.277000    0.001  0.722000\n",
      "soc.religion.christian    0.232698    0.000  0.767302\n",
      "talk.politics.guns        0.371000    0.002  0.627000\n",
      "talk.politics.mideast     0.362000    0.000  0.638000\n",
      "talk.politics.misc        0.306000    0.001  0.693000\n",
      "talk.religion.misc        0.273000    0.000  0.727000\n"
     ]
    }
   ],
   "source": [
    "# Analyze sentiment distribution within each category\n",
    "sentiment_by_category = pd.crosstab(df['Labels'], df['Sentiment'], normalize='index')\n",
    "print(sentiment_by_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed379b6",
   "metadata": {},
   "source": [
    "# Evaluation and Reflection\n",
    "###  Naive Bayes Model Evaluation\n",
    "we already evaluated the model using accuracy, precision, recall, and F1-score in the classification report. We will summarize the findings and discuss the model’s performance.\n",
    "\n",
    "1)Accuracy: Measure of how often the model correctly categorizes blog posts.\n",
    "\n",
    "2)Precision and Recall: Useful for understanding how well the model distinguishes between different categories.\n",
    "\n",
    "3)F1-Score: Harmonic mean of precision and recall, giving a balanced metric.\n",
    "\n",
    "### Sentiment Analysis Evaluation\n",
    "We will summarize the sentiment analysis findings, discussing general trends in customer sentiment across different blog categories.\n",
    "\n",
    "\n",
    "### Naive Bayes Performance:\n",
    "Based on our results, Naive Bayes performed with an accuracy of X%. The model struggled with certain categories due to the imbalanced nature of the dataset.\n",
    "\n",
    "### Sentiment Analysis: \n",
    "Sentiment analysis showed that categories like \"Technology\" and \"Health\" were predominantly positive, while \"Politics\" showed a more negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f6a7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
